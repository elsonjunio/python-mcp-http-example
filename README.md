# ğŸ§  Model Context Protocol (MCP) - Hello World

Este projeto Ã© uma implementaÃ§Ã£o de exemplo do Model Context Protocol (MCP), criado para fins de estudo e como material de referÃªncia para desenvolvedores.

## VisÃ£o Geral

O projeto demonstra como:

    Implementar um servidor MCP simples com ferramentas e recursos

    Criar um cliente MCP para interagir com o servidor

    Integrar um modelo de linguagem (LLM) para orquestrar chamadas de ferramentas


## ğŸ“¦ Estrutura do Projeto

```
.
â”œâ”€â”€ poetry.lock
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â””â”€â”€ src
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ llm_controls
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ chat_session.py
    â”‚   â”œâ”€â”€ llm_client.py
    â”‚   â””â”€â”€ mcp_http_client.py
    â”œâ”€â”€ main.py
    â””â”€â”€ server
        â”œâ”€â”€ __init__.py
        â””â”€â”€ mcp_simple_resource.py
```
## ğŸ”§ PrÃ©-requisitos

- Python 3.12+
- Poetry
- Servidor MCP rodando e acessÃ­vel via HTTP/SSE
- DependÃªncias Python instaladas

### InstalaÃ§Ã£o

```bash
git clone https://github.com/elsonjunio/python-mcp-http-example.git
cd python-mcp-http-example
poetry install
```

### Componentes Principais

1. Servidor MCP (src/server/mcp_simple_resource.py)

- Um servidor MCP simples que expÃµe:

    - Uma ferramenta add para somar dois nÃºmeros
    - VÃ¡rios recursos dinÃ¢micos e estÃ¡ticos

Como executar:

```bash
python src/server/mcp_simple_resource.py
```
O servidor estarÃ¡ disponÃ­vel em http://localhost:8000


2. Cliente MCP (src/llm_controls/mcp_http_client.py)

- Implementa a comunicaÃ§Ã£o com o servidor MCP via HTTP/SSE, permitindo:

    - Listar ferramentas disponÃ­veis
    - Executar ferramentas remotamente
    - Gerenciar recursos


3. IntegraÃ§Ã£o com LLM (src/llm_controls/chat_session.py)

- Orquestra a interaÃ§Ã£o entre:

    - O usuÃ¡rio humano
    - O modelo de linguagem (LLM)
    - As ferramentas disponÃ­veis no servidor MCP


4. Ponto de Entrada Principal (src/main.py)

Configura e inicia a sessÃ£o de chat integrada.

```bash
python src/main.py
```

## Testando o Sistema
PrÃ©-requisitos

    1 - Servidor MCP em execuÃ§Ã£o (src/server/mcp_simple_resource.py)

    2 - LM Studio (ou outro servidor LLM compatÃ­vel) executando o modelo Phi-3

## Fluxo de InteraÃ§Ã£o

    1 - Inicie o chat (python src/main.py)

    2 - Digite sua mensagem

    3 - O LLM irÃ¡:

        - Responder diretamente quando apropriado

        - Chamar a ferramenta add quando solicitado explicitamente


Exemplo de interaÃ§Ã£o:

```
You: use a ferramenta `add` com os valores 2 e 100
Final response: 
A resposta formal para o usuÃ¡rio seria: A soma dos nÃºmeros 2 e 100 Ã© igual a 102.
```

## Baseado em

Este projeto foi desenvolvido com base em:

[Python SDK do MCP](https://github.com/modelcontextprotocol/python-sdk)

[Exemplo de cliente HTTP MCP](https://github.com/slavashvets/mcp-http-client-example/tree/main)


---
## Testando Componentes Individualmente

Cada classe do projeto pode ser instanciada e testada separadamente, o que facilita o debug e o entendimento do funcionamento de cada parte:

1. Testando o LLMClient (Cliente de Modelo de Linguagem)
```python
from llm_controls.llm_client import LLMClient
import asyncio

def test_llm_client():
    """Exemplo completo para testar o LLMClient com diferentes modos de operaÃ§Ã£o"""
    client = LLMClient()
    
    print("\nğŸ”µ Modos de teste disponÃ­veis:")
    print("1 - Resposta completa (uma Ãºnica resposta)")
    print("2 - Resposta em streaming (token por token)")
    print("3 - Ambos os modos")
    choice = input("Escolha o modo de teste (1-3): ").strip()
    
    while True:
        try:
            prompt = input("\nDigite sua pergunta (ou 'sair' para encerrar): ")
            if prompt.lower() in ('sair', 'exit', 'quit'):
                break

            messages = [{"role": "user", "content": prompt}]
            
            if choice in ('1', '3'):
                print("\nğŸ”µ Resposta completa:")
                resposta = client.chat(messages)
                print(resposta)
            
            if choice in ('2', '3'):
                print("\nğŸŸ¢ Resposta em streaming:")
                for chunk in client.chat_stream(messages):
                    print(chunk, end="", flush=True)
                print()  # Nova linha apÃ³s streaming
            
        except Exception as e:
            print(f"âŒ Erro: {str(e)}")

if __name__ == '__main__':
    test_llm_client()

```

2. Testando o MCPClient (Cliente do Servidor MCP)
```python
from llm_controls.mcp_http_client import MCPClient
import asyncio
import logging
from pprint import pprint

# ConfiguraÃ§Ã£o bÃ¡sica de logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

async def test_mcp_client():
    """Exemplo completo para testar todas as funcionalidades do MCPClient"""
    try:
        # InicializaÃ§Ã£o do cliente
        mcp_client = MCPClient('test-client', 'http://localhost:8000/sse')
        await mcp_client.initialization()
        print("\nğŸŸ¢ ConexÃ£o com o servidor MCP estabelecida com sucesso!")
        
        # Listar todas as entidades disponÃ­veis
        print("\nğŸ” Listando ferramentas disponÃ­veis:")
        tools = await mcp_client.list_tools()
        pprint([tool.name for tool in tools])
        
        print("\nğŸ“¦ Listando recursos disponÃ­veis:")
        resources = await mcp_client.list_resources()
        pprint([resource.uri for resource in resources])
        
        print("\nğŸ“ Listando prompts disponÃ­veis:")
        prompts = await mcp_client.list_prompts()
        pprint([prompt.name for prompt in prompts])
        
        # Testar execuÃ§Ã£o de ferramenta (se disponÃ­vel)
        if tools and tools[0].name == 'add':
            print("\nğŸ§ª Testando execuÃ§Ã£o da ferramenta 'add':")
            result = await mcp_client.execute_tool('add', {'a': 5, 'b': 3})
            print(f"Resultado de 5 + 3 = {result}")
        
    except Exception as e:
        logging.error(f"âŒ Erro durante os testes: {str(e)}")
    finally:
        try:
            await mcp_client.cleanup()
            print("\nğŸ›‘ ConexÃ£o encerrada corretamente")
        except Exception as e:
            logging.warning(f"Aviso durante o cleanup: {str(e)}")

if __name__ == '__main__':
    asyncio.run(test_mcp_client())
```

### ComparaÃ§Ã£o com redes multiagente (arquitetura baseada em mÃºltiplos graphs)

Nos Ãºltimos meses, tenho desenvolvido sistemas de agentes utilizando [LangChain](https://python.langchain.com/docs/introduction/), observando padrÃµes comuns em implementaÃ§Ãµes corporativas:

- **Arquitetura tÃ­pica**: 
  - Dados distribuÃ­dos por setores/departamentos
  - Agentes especializados por domÃ­nio
  - Processos complexos para consolidaÃ§Ã£o de relatÃ³rios

**Principais caracterÃ­sticas do LangChain**:

âœ” Framework maduro com ampla adoÃ§Ã£o  
âœ” Capacidade de integrar diversas APIs e fontes de dados  
âœ” Mecanismos robustos para orquestraÃ§Ã£o de agentes  
âœ” Prompts especializados por contexto/domÃ­nio  

**Sobre o MCP** ([Model Context Protocol](https://spec.modelcontextprotocol.io/specification/2025-03-26/)):

ğŸ”§ Protocolo aberto para padronizaÃ§Ã£o de comunicaÃ§Ã£o LLM-serviÃ§os  
ğŸš€ Foco em interoperabilidade entre sistemas  
ğŸ§© Permite construÃ§Ã£o de agentes complexos com orquestraÃ§Ã£o nativa  

**CenÃ¡rios de uso complementares**:
1. **MigraÃ§Ã£o gradual**: Adicionar novos componentes via MCP em sistemas LangChain existentes
2. **Arquitetura hÃ­brida**: 
   - LangChain para orquestraÃ§Ã£o principal
   - MCP para integraÃ§Ã£o com serviÃ§os especializados
3. **PadronizaÃ§Ã£o**: Utilizar MCP como camada de abstraÃ§Ã£o para serviÃ§os heterogÃªneos

**Vantagens da abordagem combinada**:
- ReduÃ§Ã£o de technical debt em integraÃ§Ãµes customizadas
- Maior flexibilidade para substituir componentes
- Possibilidade de reutilizaÃ§Ã£o entre diferentes frameworks

